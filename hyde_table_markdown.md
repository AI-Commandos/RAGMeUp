### Question 1: What are the main considerations or concerns of the LDA paper, and who does the paper reference for explaining the main concept of LDA?

| Field                  | With HYDE                                   | Without HYDE                                |
|------------------------|---------------------------------------------|---------------------------------------------|
| **Documents Found**    | document nr: 0 <br> source: data/Week 6 reading b -2021-SSCI-AComparativeStudyOfFuzzyTopicModelsAndLDAInTermsOfInterpretability.pdf <br> provenance: 0.993 <br> [3] Blei, D. M., Ng, A. Y ., & Jordan, M. I. (2003). Latent dirichlet allocation. the Journal of machine Learning research, 3, 993-1022. [4] Blei, D. M., & Lafferty, J. D. (2006). Dynamic topic models. In Proceedings of the 23rd international conference on Machine learning (pp. 113-120). [5] Blei, D., & Lafferty, J. (2006). Correlated topic models. Advances in neural information processing systems, 18, 147. [6] Blei, D. M. (2012). Probabilistic topic models. Communications of the ACM, 55(4), 77-84. <br><br>document nr: 1 <br> source: data/week 6.pdf <br> provenance: 0.988 <br> structure of the documents and extract topics (e.g. BERTopic). 1) Blei, D. M., Ng, A. Y., & Jordan, M. I. (2003). Latent dirichlet allocation. Journal of machine Learning research, 3(Jan), 993-1022. 2) Grootendorst, M. (2022). BERTopic: Neural topic modeling with a class-based TF-IDF procedure. arXiv preprint arXiv:2203.05794. 3) Landauer, T. K., Foltz, P. W., & Laham, D. (1998). An introduction to latent semantic analysis. Discourse processes, 25(2-3), 259-284. <br><br>document nr: 2 <br> source: data/Week 6 reading d - LatenDirichletAllocation (1).pdf <br> provenance: 0.981 <br> text modeling, the topic probabilities provide an explicit representation of a document. We present efﬁcient approximate inference techniques based on variational methods and an EM algorithm for empirical Bayes parameter estimation. We report results in document modeling, text classiﬁcation, and collaborative ﬁltering, comparing to a mixture of unigrams model and the probabilistic LSI model. 1. Introduction In this paper we consider the problem of modeling text corpora and other collections of discrete <br><br>document nr: 3 <br> source: data/week 6.pdf <br> provenance: 0 <br> JM2050 Natural Language Processing 2024 –2025 Uzay Kaymak 82 We follow a three-step approach  1. Create topics (LDA & FLSA-W)* 2. Interpreting Topics with a Domain Expert and ChatGPT 3. Compare the generated summaries with those  produced by a domain expert • Blei, D. M., Ng, A. Y., & Jordan, M. I. (2003). Latent dirichl et allocation.Journal of machine Learning research, 3(Jan), 993-1022. <br><br>                            | document nr: 0 <br> source: data/Week 3 reading c - Mykolo etal (2013)-Word2Vec.pdf <br> provenance: 0.97 <br> the most successful concept is to use distributed representations of words [10]. For example, neural network based language models signiﬁcantly outperform N-gram models [1, 27, 17]. 1.1 Goals of the Paper The main goal of this paper is to introduce techniques that can be used for learning high-quality word vectors from huge data sets with billions of words, and with millions of words in the vocabulary. As far as we know, none of the previously proposed architectures has been successfully trained on more 1 <br><br>document nr: 1 <br> source: data/Week 6 reading d - LatenDirichletAllocation (1).pdf <br> provenance: 0.957 <br> conclusions. 2. Notation and terminology We use the language of text collections throughout the paper, referring to entities such as “words,” “documents,” and “corpora.” This is useful in that it helps to guide intuition, particularly when we introduce latent variables which aim to capture abstract notions such as topics. It is important to note, however, that the LDA model is not necessarily tied to text, and has applications to other <br><br>document nr: 2 <br> source: data/Week 7 reading .pdf <br> provenance: 0.89 <br> supervised labels speciﬁc to the desired task. The main advantage of ﬁne-tuning is strong performance on many benchmarks. The main disadvantages are the need for a new large dataset for every task, the potential for poor generalization out-of-distribution [MPL19], and the potential to exploit spurious features of the training data [GSL+18, NK19]. We focus on task-agnostic performance, leaving ﬁne-tuning for future work. <br><br>document nr: 3 <br> source: data/Week 9 deep_learning_1.pdf <br> provenance: 0 <br> GloVeHey! This is something that we do in LSA/LDA too! <br><br>document nr: 4 <br> source: data/Week 2 reading - 2_TextProc_2023.pdf <br> provenance: 0 <br> How is lemmatization done? The most sophisticated methods for lemmatization involve complete morphological parsing of the word. Morphology is the study of the way words are built up from smaller meaning-bearing units called morphemes .morpheme Two broad classes of morphemes can be distinguished: stems —the central mor-stem pheme of the word, supplying the main meaning— and afﬁxes —adding “additional”afﬁx meanings of various kinds. So, for example, the word fox consists of one morpheme <br><br>document nr: 5 <br> source: data/Week 6 reading d - LatenDirichletAllocation (1).pdf <br> provenance: 0 <br> alternative approach is the “folding-in” heuristic suggested by Hofmann (1999), where one ignores thep (z \|d ) parameters and reﬁts p (z \|d new ). Note that this gives the pLSI model an unfair advantage by allowing it to reﬁt k −1 parameters to the test data. LDA suffers from neither of these problems. As in pLSI, each document can exhibit a different proportion of underlying topics. However, LDA can easily assign probability to a new document; <br><br>document nr: 6 <br> source: data/Week 7 reading .pdf <br> provenance: 0 <br> allowed the model to naturally carry out completions and created a corpus of such completions for studying co-occurrence of words. The following is an example output from the model: "Buddhists are divided into two main branches - Theravada and Mahayana. Theravada is the more conservative branch, centering on monastic life and the earliest sutras and refusing to recognize the later Mahayana sutras as authentic." <br><br>document nr: 7 <br> source: data/week 6.pdf <br> provenance: 0 <br> • Rijcken, E., Scheepers, F., Mosteiro, P., Zervanou, K., Spruit , M., & Kaymak, U. (2021). A comparative study of fuzzy topic models  and LDA in terms of interpretability. In 2021 IEEE Symposium Series on Computational Intelligence (SSCI) (pp. 1-8). IEEE. <br><br>document nr: 8 <br> source: data/Week 8 Tutorial Topic Modeling.pdf <br> provenance: 0 <br> # Print the topics for topic in lda_model.print_topics():     print(topic) #Get coherence score print(CoherenceModel(model=lda_model, texts=data, dictionary=dictionary, coherence='c_v').get_coherence()) <br><br>document nr: 9 <br> source: data/Week 12 aGAI_ChatGPT_LLM.pdf <br> provenance: 0 <br> UnderstandLing LLMs and their applications J A D S E R I K  T R O M P  - U N D E R S T A N D L I N G <br><br>document nr: 10 <br> source: data/Week 6 reading d - LatenDirichletAllocation (1).pdf <br> provenance: 0 <br> L A TENT D IRICHLET A LLOCA TION β α zwθ N M zθ φγ N M Figure 5: (Left) Graphical model representation of LDA. (Right) Graphical model representation of the variational distribution used to approximate the posterior in LDA. 5.1 Inference The key inferential problem that we need to solve in order to use LDA is that of computing the posterior distribution of the hidden variables given a document: p (θ , z \|w , α , β )= p (θ , z , w \|α , β ) p (w \|α , β ) . <br><br>document nr: 11 <br> source: data/Week 6 reading b -2021-SSCI-AComparativeStudyOfFuzzyTopicModelsAndLDAInTermsOfInterpretability.pdf <br> provenance: 0 <br> relationships [47] and has been used before as a topic embed- ding in the medical domain [29], [40]. LDA is a probabilistic model and assumes that documents are formed by a generative process [3]. Each document is a distribution over C topics in this process, and each topic is a distribution over M words. Thus, if document j has a high probability of containing topic k, then topic k’s most probable words are likely to be present in document j. Yet, both the distributions of topics <br><br>document nr: 12 <br> source: data/week 6.pdf <br> provenance: 0 <br> JM2050 Natural Language Processing 2024 –2025 Uzay Kaymak 15 Latent Dirichlet Allocation (LDA) LDA assumes documents can be created by sampling from: - a distribution of topics over documents.- a distribution of words over topics.  15 <br><br>document nr: 13 <br> source: data/Week 6 reading b -2021-SSCI-AComparativeStudyOfFuzzyTopicModelsAndLDAInTermsOfInterpretability.pdf <br> provenance: 0 <br> Also, FLSA-W’s coherence scores are higher for almost all the number of topics, with 20-word-topics. These changes indicate that the number of words per topic may impact a topic’s quality, depending on the used topic model. 3) LDA has a much higher variation in coherence score than the other models for almost all settings. This indi- cates that our proposed models and FLSA are more sta- ble topic models than LDA, since there is less variability in the models that they produce. The above ﬁndings are <br><br>                         |
| **LLM's Final Answer** | According to Document 0, the main considerations or concerns of the LDA paper are not explicitly mentioned. However, it is clear that the authors aimed to introduce a new method for topic modeling.<br><br>To explain the main concept of LDA, the authors reference the concept of "Latent Dirichlet Allocation" itself, which implies that they are building upon existing knowledge of Dirichlet distributions and allocation. <br><br>However, if we look at Document 1, which references the original LDA paper by Blei, Ng, and Jordan (2003), it seems that the authors of the LDA paper do not reference any specific work for explaining the main concept of LDA. Instead, they seem to be introducing a novel approach to topic modeling.<br><br>But if we take into account the context provided by Document 3, where the authors mention that they are using LDA and another technique called FLSA-W, it suggests that the main concern of the LDA paper might be its ability to create topics that can be interpreted by a domain expert.<br><br>It is also worth noting that the authors of the LDA paper do not mention any specific references for explaining the main concept of LDA, but rather focus on presenting their own contribution to the field of topic modeling.                          | According to Document 10, the main inferential problem that needs to be solved in order to use LDA is computing the posterior distribution of the hidden variables given a document: p(θ, z \| w, α, β) = p(θ, z, w \| α, β) / p(w \| α, β).<br><br>However, the main considerations or concerns of the LDA paper are not explicitly stated in the provided documents.<br><br>But according to Document 11, LDA assumes that documents are formed by a generative process, where each document is a distribution over C topics, and each topic is a distribution over M words. <br><br>As for the references, the papers referenced in the provided documents do not provide information about the authors of the original LDA paper. However, the authors mentioned in the provided documents are:<br><br>* Blei, David M., Ng, Andrew Y., and Jordan, Michael I. (2003) - although this is not directly mentioned in any of the provided documents but is widely known as the originators of the LDA algorithm.<br><br>The provided documents also mention other researchers' works like Hofmann (1999), GSL+18, NK19, MPL19, Rijcken et al. (2021), etc., but they are not the original authors of the LDA concept.                       |
| **Time Taken**         | 46.72 seconds                            | 23.69 seconds                         |

### Question 2: What are the main challenges of prompt engineering according to research?

| Field                  | With HYDE                                   | Without HYDE                                |
|------------------------|---------------------------------------------|---------------------------------------------|
| **Documents Found**    | document nr: 0 <br> source: data/Week 6 reading a - BERTopic-NeuralTopicModelingWithAClassBasedTF-IDFprocedure (1).pdf <br> provenance: 0.993 <br> 2020). CTM, for example, demonstrates the ad- vantage of relying on pre-trained language models, namely that future improvements in language mod- els may translate into better topic models (Bianchi et al., 2020a). Several approaches have started simplifying the topic building process by clustering word- and document embeddings (Sia et al., 2020; Angelov, 2020). This clustering approach allows for a ﬂex- ible topic model as the generation of the clusters can be separated from the process of generating the <br><br>document nr: 1 <br> source: data/Week 6 reading a - BERTopic-NeuralTopicModelingWithAClassBasedTF-IDFprocedure (1).pdf <br> provenance: 0.989 <br> score is the average of 15 separate runs. model for BERTopic and CTM (Song et al., 2020). Two variations of Top2Vec were modeled, one with Doc2Vec and one with the "all-mpnet-base-v2" SBERT model7. For fair comparisons between BERTopic and Top2Vec, the parameters of HDBSCAN and UMAP were ﬁxed between topic models. To measure the generalizability of BERTopic across language models, four different language models were used in the experiments with BERTopic, namely the Universal Sentence Encoder <br><br>document nr: 2 <br> source: data/Week 6 reading c 2023-IFSA-InterpretingTopicModelsWithChatGPT.pdf <br> provenance: 0.915 <br> trained using a language modeling objective, but it performs no fine-tuning [28]. Instead, it uses textual prompts to perform zero-shot inference on various tasks. Ever since the switch from the pre-train and fine-tune paradigm to the pre-train, prompt, and predict paradigm [22], there is a growing literature on prompt engineering [29], [30]. After the GPT-2 proposal, evidence suggested that LM performance strongly depends on the scale and only weakly on the model shape and that LMs are <br><br>document nr: 3 <br> source: data/Week 7.pdf <br> provenance: 0 <br> reasoning paths that can recover the answer. Idea: sample multiple, diverse reasoning paths through few-shot CoT, and  use the generations to select the most consistent answer. 61 Wang, X., Wei, J., Schuurmans, D., Le, Q., Chi, E., Narang, S., ... & Zhou, D. (2022). Self-Consistency Improves Chain of Thought  Reasoning in Language Models. In Proceedings of the International Conference on Learning Representations (ICLR). <br><br>                            | document nr: 0 <br> source: data/Week 7.pdf <br> provenance: 0.942 <br> JM2050 Natural Language Processing 2024 – 2025 U. Kaymak 35 Advanced prompt engineering techniques - Few shot prompts - Chain of thought reasoning - Effective graph of thought https://arxiv.org/abs/2305.16582 - Self-consistency https://openreview.net/forum?id=1PL1NIMMrw - Knowledge generation prompting -R e A C T 41 <br><br>document nr: 1 <br> source: data/Week 7.pdf <br> provenance: 0.882 <br> JM2050 Natural Language Processing 2024 – 2025 U. Kaymak 19 Prompts and prompt engineering 19  Source: https://www.cse.iitd.ac.in/~mausam/courses/col772/spring2023/lectures/22-promptengg.pdf <br><br>document nr: 2 <br> source: data/Week 7.pdf <br> provenance: 0.813 <br> JM2050 Natural Language Processing 2024 – 2025 U. Kaymak 31 Advanced Prompt  Engineering <br><br>document nr: 3 <br> source: data/Week 5 guest a[Xudong Lu]LLM-medicine Research Overview.pdf <br> provenance: 0 <br> Discussions of Breast-COMA ➢ Privacy issue: Using the ChatGPT API resulted in patients’ data being public, violating patients’ privacy rights. ➢ Security issue: LLM lacks professionalism and interpretability, posing potential security risks to patients. ➢ Prompt engineering limitations: Prompt design lacks clear optimization directions and relies on human experience. Few-shot is extremely sensitive to example selection, distribution, and design. It is difficult for LLM to balance accuracy and cost. <br><br>document nr: 4 <br> source: data/Week 7 reading .pdf <br> provenance: 0.89 <br> supervised labels speciﬁc to the desired task. The main advantage of ﬁne-tuning is strong performance on many benchmarks. The main disadvantages are the need for a new large dataset for every task, the potential for poor generalization out-of-distribution [MPL19], and the potential to exploit spurious features of the training data [GSL+18, NK19]. We focus on task-agnostic performance, leaving ﬁne-tuning for future work. <br><br>document nr: 5 <br> source: data/Week 4 reading b - 24_Dialogue_May_6_2021.pdf <br> provenance: 0 <br> Dialog System Design: User-centered Design1.Study the users and task•value-sensitive design2.Build simulations•Wizard of Oz study3.Iteratively test design on users Gould, John D., and Clayton Lewis. "Designing for usability: key principles and what designers think."Communications of the ACM28, no. 3 (1985): 300-311.Bender, Emily M., and BatyaFriedman. "Data statements for natural language processing: Toward mitigating system bias and enabling better science." TACL 6 (2018): 587-604. <br><br>document nr: 6 <br> source: data/Week 6 reading c 2023-IFSA-InterpretingTopicModelsWithChatGPT.pdf <br> provenance: 0 <br> language model ChatGPT. Our approach is built on a three- stage process where we first use topic modeling to identify the main topics in the corpus. Then, we ask a domain expert to assign themes to these topics and prompt ChatGPT to generate human- readable summaries of the topics. Lastly, we compare the human- and machine-produced interpretations. The domain expert found half of ChatGPT’s descriptions useful. This explorative work demonstrates ChatGPT’s capability to describe topics accurately <br><br>document nr: 7 <br> source: data/Week 6 reading c 2023-IFSA-InterpretingTopicModelsWithChatGPT.pdf <br> provenance: 0 <br> customization process per prompt to obtain more specific topic descriptions. Moreover, we plan to include more domain experts, change the questions to reduce bias and conduct our experiments on a larger scale. <br><br>document nr: 8 <br> source: data/Week 7.pdf <br> provenance: 0 <br> prompts format: <input, chain of thought, output> <br><br>document nr: 9 <br> source: data/Week 5 guest b[Shan Nan]Extracting principal diagnosis from discharge summary using deep learning and LLMs.pdf <br> provenance: 0 <br> How does the LLM Approach perform? • Not as good as expected • Retrain and/or  instruction tuning need  excessive computing  power An Experiment ChatGPT ChatGPT- constraint HR@5 0.39 0.54 HR@1 0.20 0.28 Randomly select  100 discharge  summaries from  EMR Prompt  Engineering 2 1 Directly ask ChatGPT 3.5 Turbo  without prompt design <br><br>document nr: 10 <br> source: data/Week 6 reading c 2023-IFSA-InterpretingTopicModelsWithChatGPT.pdf <br> provenance: 0 <br> [29] S. Arora, A. Narayan, M. F. Chen, L. J. Orr, N. Guha, K. Bhatia, I. Chami, F. Sala, and C. R ´e, “Ask me anything: A simple strategy for prompting language models,” arXiv preprint arXiv:2210.02441 , 2022. [30] J. White, Q. Fu, S. Hays, M. Sandborn, C. Olea, H. Gilbert, A. El- nashar, J. Spencer-Smith, and D. C. Schmidt, “A prompt pattern catalog to enhance prompt engineering with chatgpt,” arXiv preprint arXiv:2302.11382, 2023. <br><br>document nr: 11 <br> source: data/Week 7.pdf <br> provenance: 0 <br> JM2050 Natural Language Processing 2024 – 2025 U. Kaymak 37 Few-Shot Prompts Key findings: • Ground truth in prompts matters little. Instead, other key asp ects of the demonstrations are  key drivers in end task performance: 1. The label space2. The distribution of the input text3. The overall format of the sequence.  43 Min, S. et al.  (2022). study how the model learns and which aspects of the demonstrations  contribute to end task performance. <br><br>document nr: 12 <br> source: data/Week 7.pdf <br> provenance: 0 <br> JM2050 Natural Language Processing 2024 – 2025 U. Kaymak 32 38 Machine Learning methods have greatly been inspired by natural phenomena • Neural Networks • Genetic Algorithms• Particle Swarm Optimization• Ant Colony Optimization• Bee algorithms• Fuzzy sets Many prompting techniques are inspired by the human-thought process <br><br>                         |
| **LLM's Final Answer** | According to Document 2, the main challenge of prompt engineering is that the performance of language models (LMs) strongly depends on the scale rather than the model shape, suggesting that finding the optimal scale is crucial for effective prompt engineering.<br><br>Source: Document 2, id: 007371725d9f1cf89f88b9e1221dc82d, source: data/Week 6 reading c 2023-IFSA-InterpretingTopicModelsWithChatGPT.pdf                          | According to Document 3, discussions of Breast-COMA, the main challenges of prompt engineering are:<br><br>1. **Privacy issue**: Using the ChatGPT API resulted in patients' data being public, violating patients' privacy rights.<br>2. **Security issue**: LLM lacks professionalism and interpretability, posing potential security risks to patients.<br>3. **Prompt engineering limitations**: Prompt design lacks clear optimization directions and relies on human experience. Few-shot is extremely sensitive to example selection, distribution, and design.<br><br>Additionally, Document 11 mentions that ground truth in prompts matters little, and instead, other key aspects of the demonstrations are key drivers in end-task performance, such as:<br><br>1. **Label space**<br>2. **Distribution of the input text**<br>3. **Overall format of the sequence**<br><br>These challenges highlight the complexities and nuances involved in designing effective prompts for language models like ChatGPT.                       |
| **Time Taken**         | 37.87 seconds                            | 16.26 seconds                         |

### Question 3: What are the different types of topic modeling 

| Field                  | With HYDE                                   | Without HYDE                                |
|------------------------|---------------------------------------------|---------------------------------------------|
| **Documents Found**    | document nr: 0 <br> source: data/week 6.pdf <br> provenance: 0.988 <br> structure of the documents and extract topics (e.g. BERTopic). 1) Blei, D. M., Ng, A. Y., & Jordan, M. I. (2003). Latent dirichlet allocation. Journal of machine Learning research, 3(Jan), 993-1022. 2) Grootendorst, M. (2022). BERTopic: Neural topic modeling with a class-based TF-IDF procedure. arXiv preprint arXiv:2203.05794. 3) Landauer, T. K., Foltz, P. W., & Laham, D. (1998). An introduction to latent semantic analysis. Discourse processes, 25(2-3), 259-284. <br><br>document nr: 1 <br> source: data/Week 6 reading b -2021-SSCI-AComparativeStudyOfFuzzyTopicModelsAndLDAInTermsOfInterpretability.pdf <br> provenance: 0.983 <br> [3] Blei, D. M., Ng, A. Y ., & Jordan, M. I. (2003). Latent dirichlet allocation. the Journal of machine Learning research, 3, 993-1022. [4] Blei, D. M., & Lafferty, J. D. (2006). Dynamic topic models. In Proceedings of the 23rd international conference on Machine learning (pp. 113-120). [5] Blei, D., & Lafferty, J. (2006). Correlated topic models. Advances in neural information processing systems, 18, 147. [6] Blei, D. M. (2012). Probabilistic topic models. Communications of the ACM, 55(4), 77-84. <br><br>document nr: 2 <br> source: data/Week 6 reading b -2021-SSCI-AComparativeStudyOfFuzzyTopicModelsAndLDAInTermsOfInterpretability.pdf <br> provenance: 0.87 <br> indicating the most important topics for its decisions. A problem with current topic models is they do not always give interpretable results. Fuzzy Latent Semantic Analysis (FLSA) [19] is a topic modeling algorithm applied to medical data and showed superior performance to the most popular model Latent Dirichlet Allocation (LDA). FLSA is based on grouping similar documents in terms of the words they contain. The membership of each document to a topic is <br><br>document nr: 3 <br> source: data/Week 6 reading c 2023-IFSA-InterpretingTopicModelsWithChatGPT.pdf <br> provenance: 0 <br> FuzzyTM package [15] packages to train LDA and FLSA-W, respectively. FLSA-W is a fuzzy topic modeling algorithm that considers the degree of membership of a word to a topic. LDA is a widely used probabilistic topic modeling algorithm that infers the underlying topic structure of a corpus based on the distribution of words. 2) Step 2 - Interpreting Topics with a Domain Expert and ChatGPT: In this step, we ask a human domain expert and ChatGPT to interpret the topics generated in the first <br><br>                            | document nr: 0 <br> source: data/Week 6 reading b -2021-SSCI-AComparativeStudyOfFuzzyTopicModelsAndLDAInTermsOfInterpretability.pdf <br> provenance: 0.976 <br> The contributions of this paper are: 1) we propose two new topic modeling algorithms, 2) we compare four different topic models in terms of interpretability, 3) we study the relation between various parameters of the topic models and their properties that can could be useful 978-1-7281-9048-8/21/$31.00 ©2021 IEEE <br><br>document nr: 1 <br> source: data/Week 6 reading b -2021-SSCI-AComparativeStudyOfFuzzyTopicModelsAndLDAInTermsOfInterpretability.pdf <br> provenance: 0.956 <br> determined by using fuzzy clustering. However, documents might contain different number of topics and may contain different subsets of words in a topic, which inﬂuences the quality of the clustering results. This leads typically to large overlap between topics, which reduces the interpretability of the topic model obtained. In this paper, we propose two topic modeling methods that can reduce the overlap between the topics. The ﬁrst model, FLSA-W, clusters on words rather <br><br>document nr: 2 <br> source: data/Week 9 deep_learning_1.pdf <br> provenance: 0.904 <br> Auto-encodersBut you can also do topic modeling with this <br><br>document nr: 3 <br> source: data/Week 6 reading c 2023-IFSA-InterpretingTopicModelsWithChatGPT.pdf <br> provenance: 0 <br> the potential of using ChatGPT, the state-of-the-art in LLMs, to interpret the output generated by topic models. We work with topics trained on Electronic Health Records (EHRs) and compare the interpretations from ChatGPT and a domain expert. We follow a three-step approach. In the first step, we create topics generated by the popular probabilistic topic modeling algorithm Latent Dirichlet Allocation (LDA) [12]. Because of the nature of topic modeling, where words belong to different <br><br>document nr: 4 <br> source: data/Week 6 reading b -2021-SSCI-AComparativeStudyOfFuzzyTopicModelsAndLDAInTermsOfInterpretability.pdf <br> provenance: 0 <br> based topic model [37]. For documents with more words, the Topics over Time model [48] and Dynamic Topic Model [4] are suitable for capturing changes of topics over time. The Pachinko Allocation Model [22] and the Correlated Topic Model [5] are suitable when there is an interest in the correlation between topics. The most popular topic modelling technique is called Latent Dirichlet Allocation (LDA) [3]. LDA performs well on data documents with more than 50 words and no complex topic <br><br>document nr: 5 <br> source: data/week 6.pdf <br> provenance: 0 <br> JM2050 Natural Language Processing 2024 –2025 Uzay Kaymak 6 What are topic models Topic model = an (unsupervised) statistical model to discover latent  topics from a corpus. 6 Input: corpus + number of topics Output: Two matrices: 1. P(word\|topic) (words x topics) 2. P(topic\|document)    (topics x documents) Per topic, the top nwords are typically used to  represent a topic Traditional topic modeling  algorithms return two matrices. This need not be the case for all models. <br><br>document nr: 6 <br> source: data/week 6.pdf <br> provenance: 0 <br> JM2050 Natural Language Processing 2024 –2025 Uzay Kaymak 4 What is a topic model 4 <br><br>document nr: 7 <br> source: data/Week 12 b deep_learning_3.pdf <br> provenance: 0 <br> Attention modelsExample for topic modelling <br><br>document nr: 8 <br> source: data/Week 6 reading b -2021-SSCI-AComparativeStudyOfFuzzyTopicModelsAndLDAInTermsOfInterpretability.pdf <br> provenance: 0 <br> to represent texts numerically. Topic models are a different branch of NLP techniques, and their output consists of two matrices: 1) P(Wi\|Tk)- The probability of word i given topic k, 2) P(Tk\|Dj)- The probability of topic k given document j with: i word index i ∈{1, 2, 3, ..., M}, j document index j ∈{1, 2, 3, ..., N}, k topic index k ∈{1, 2, 3, ..., C}, M the number of unique words in the data set, N the number of documents in the data set, C the number of topics. <br><br>document nr: 9 <br> source: data/week 6.pdf <br> provenance: 0 <br> Approach: Train each algorithm with  A number of topics 5, 10, … , 100 Rijcken, E., Kaymak, U., Scheepers, F., Mosteiro, P., Zervanou, K., & Spruit, M. (2022). Topic Modeling  for Interpretable Text Classification From EHRs. Frontiers in Big Data, 5. <br><br>document nr: 10 <br> source: data/Week 6 reading a - BERTopic-NeuralTopicModelingWithAClassBasedTF-IDFprocedure (1).pdf <br> provenance: 0 <br> narrative in text, topic models have proven to be a powerful unsupervised tool. Conventional models, such as Latent Dirichlet Allocation (LDA) (Blei et al., 2003) and Non-Negative Matrix Factoriza- tion (NMF) (Févotte and Idier, 2011), describe a document as a bag-of-words and model each docu- ment as a mixture of latent topics. One limitation of these models is that through bag-of-words representations, they disregard se- mantic relationships among words. As these repre- <br><br>document nr: 11 <br> source: data/Week 6 reading c 2023-IFSA-InterpretingTopicModelsWithChatGPT.pdf <br> provenance: 0 <br> and provide useful insights if prompted accurately. Index Terms—Topic Modeling, LLM, ChatGPT, Electronic Health Records, Fuzzy Topic Models, Prompt Engineering I. I NTRODUCTION As the volume of textual data continues to grow, so does the significance of extracting valuable insights from large text corpora. Topic modeling is a technique for identifying latent semantic structures within vast volumes of text. Topic models are a group of unsupervised natural language processing <br><br>document nr: 12 <br> source: data/Week 6 reading b -2021-SSCI-AComparativeStudyOfFuzzyTopicModelsAndLDAInTermsOfInterpretability.pdf <br> provenance: 0 <br> The ﬁrst step in developing a topic-based text classiﬁcation algorithm is selecting a suitable topic model. A good topic model should both be interpretable and leading to high classiﬁ- cation performance. Typically, classiﬁcation models use many (at least 50) topics [19], [40]. Therefore, topic models must remain coherent as the number of topics increase. Many different topic models exist, all having a speciﬁc purpose [47]. Applicable topic models for documents with less <br><br>                         |
| **LLM's Final Answer** | Based on the provided documents, there are several types of topic modeling mentioned.<br><br>According to Document 1, "Latent Dirichlet Allocation" (LDA) is a type of topic modeling algorithm developed by Blei et al. (2003) [1]. It is a probabilistic model that infers the underlying topic structure of a corpus based on the distribution of words.<br><br>Document 2 mentions "Fuzzy Latent Semantic Analysis" (FLSA) as another type of topic modeling algorithm, which is an improvement over traditional LDA. FLSA groups similar documents based on the words they contain and provides more interpretable results.<br><br>Additionally, Document 3 introduces "Fuzzy Topic Modeling" (FTM) algorithms, such as FLSA-W, which considers the degree of membership of a word to a topic. This approach allows for more nuanced understanding of the relationships between words and topics.<br><br>Furthermore, Document 1 also mentions "Correlated Topic Models" (CTMs) developed by Blei and Lafferty (2006) [5], which extend the basic LDA model by incorporating correlations between topics.<br><br>Lastly, Document 1 references "Dynamic Topic Models" (DTMs) proposed by Blei and Lafferty (2006) [4], which allow for time-evolving topics and are useful for analyzing temporal data.<br><br>These are some of the different types of topic modeling techniques mentioned in the provided documents.                          | According to Document 4, some of the popular topic modeling techniques include:<br><br>* Latent Dirichlet Allocation (LDA) [3]<br>* Topics over Time model [48]<br>* Dynamic Topic Model [4]<br>* Pachinko Allocation Model [22]<br>* Correlated Topic Model [5]<br><br>Additionally, Document 1 mentions two proposed topic modeling methods that can reduce overlap between topics: FLSA-W and another method.<br><br>Furthermore, Document 5 explains that traditional topic modeling algorithms return two matrices, but this need not be the case for all models.<br><br>Lastly, Document 8 describes the output of topic models as consisting of two matrices: P(Wi\|Tk) and P(Tk\|Dj).<br><br>These are some examples of the different types of topic modeling techniques mentioned in the provided documents.                       |
| **Time Taken**         | 55.43 seconds                            | 16.12 seconds                         |

### Question 4: What are the limitations of BERTopic?

| Field                  | With HYDE                                   | Without HYDE                                |
|------------------------|---------------------------------------------|---------------------------------------------|
| **Documents Found**    | document nr: 0 <br> source: data/Week 6 reading b -2021-SSCI-AComparativeStudyOfFuzzyTopicModelsAndLDAInTermsOfInterpretability.pdf <br> provenance: 0.997 <br> ACM, 55(4), 77-84. [7] Devlin, J., Chang, M., Lee, K., & Toutanova, K. (2019). BERT: Pre- training of Deep Bidirectional Transformers for Language Understand- ing. Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, V olume 1 (4171–4186) [8] Dieng, A. B., Ruiz, F. J., & Blei, D. M. (2020). Topic modeling in embedding spaces. Transactions of the Association for Computational Linguistics, 8, 439-453. <br><br>document nr: 1 <br> source: data/Week 6 reading b -2021-SSCI-AComparativeStudyOfFuzzyTopicModelsAndLDAInTermsOfInterpretability.pdf <br> provenance: 0.992 <br> [48] Wang, X., & McCallum, A. (2006). Topics over time: a non-markov continuous-time model of topical trends. In Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining (pp. 424-433). [49] Zuo, Y ., Wu, J., Zhang, H., Lin, H., Wang, F., Xu, K., & Xiong, H. (2016). Topic modeling of short texts: A pseudo-document view. In Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining (pp. 2105-2114). <br><br>document nr: 2 <br> source: data/Week 6 reading a - BERTopic-NeuralTopicModelingWithAClassBasedTF-IDFprocedure (1).pdf <br> provenance: 0.979 <br> score is the average of 15 separate runs. model for BERTopic and CTM (Song et al., 2020). Two variations of Top2Vec were modeled, one with Doc2Vec and one with the "all-mpnet-base-v2" SBERT model7. For fair comparisons between BERTopic and Top2Vec, the parameters of HDBSCAN and UMAP were ﬁxed between topic models. To measure the generalizability of BERTopic across language models, four different language models were used in the experiments with BERTopic, namely the Universal Sentence Encoder <br><br>document nr: 3 <br> source: data/Week 3 reading c - Mykolo etal (2013)-Word2Vec.pdf <br> provenance: 0 <br> [3] T. Brants, A. C. Popat, P. Xu, F. J. Och, and J. Dean. Large language models in machine translation. In Proceedings of the Joint Conference on Empirical Methods in Natural Language Processing and Computational Language Learning, 2007. [4] R. Collobert and J. Weston. A Uniﬁed Architecture for Natural Language Processing: Deep Neural Networks with Multitask Learning. In International Conference on Machine Learning, ICML, 2008. <br><br>                            | document nr: 0 <br> source: data/Week 7 reading .pdf <br> provenance: 0.98 <br> matching the performance of state-of-the-art ﬁne-tuned systems, as well as generating high-quality samples and strong qualitative performance at tasks deﬁned on-the-ﬂy. We documented roughly predictable trends of scaling in performance without using ﬁne-tuning. We also discussed the social impacts of this class of model. Despite many limitations and weaknesses, these results suggest that very large language models may be an important ingredient in the development of adaptable, general language systems. <br><br>document nr: 1 <br> source: data/Week 6 reading a - BERTopic-NeuralTopicModelingWithAClassBasedTF-IDFprocedure (1).pdf <br> provenance: 0.97 <br> perspective. In this paper, we introduce BERTopic, a topic arXiv:2203.05794v1  [cs.CL]  11 Mar 2022 <br><br>document nr: 2 <br> source: data/Week 7 reading .pdf <br> provenance: 0.917 <br> Our goal is not to exhaustively characterize GPT-3, but to give a preliminary analysis of some of its limitations and behaviors. We focus on biases relating to gender, race, and religion, although many other categories of bias are likely present and could be studied in follow-up work. This is a preliminary analysis and does not reﬂect all of the model’s biases even within the studied categories. Broadly, our analysis indicates that internet-trained models have internet-scale biases; models tend to <br><br>document nr: 3 <br> source: data/Week 7.pdf <br> provenance: 0 <br> JM2050 Natural Language Processing 2024 – 2025 U. Kaymak 67 Limitations and considerations of ToT 74 Disadvantage: Computational requirements: at worst, ToT has an exponential  number of calls with increasing lengths per call.  Considerations: Deliberate search/reasoning methods might not be necessary for  many existing tasks.  However, as LMs are being deployed for more real-world decision  making applications, more complex tasks could emerge and present new opportunities to study these research questions. <br><br>document nr: 4 <br> source: data/Week 3 reading c - Mykolo etal (2013)-Word2Vec.pdf <br> provenance: 0 <br> normalization. 2.2 Recurrent Neural Net Language Model (RNNLM) Recurrent neural network based language model has been proposed to overcome certain limitations of the feedforward NNLM, such as the need to specify the context length (the order of the modelN), and because theoretically RNNs can efﬁciently represent more complex patterns than the shallow neural networks [15, 2]. The RNN model does not have a projection layer; only input, hidden and <br><br>document nr: 5 <br> source: data/Week 6 reading a - BERTopic-NeuralTopicModelingWithAClassBasedTF-IDFprocedure (1).pdf <br> provenance: 0 <br> IDF, we can represent topics as a distribution of words. These distributions have allowed BERTopic to model the dynamic and evolutionary aspects of topics with little changes to the core algorithm. Similarly, with these distributions, we can also model the representations of topics across classes. 7.2 Weaknesses No model is perfect and BERTopic is deﬁnitely no exception. There are several weaknesses to the model that should be addressed. First, BERTopic assumes that each document only contains a sin- <br><br>document nr: 6 <br> source: data/Week 4 tutorial.pdf <br> provenance: 0 <br> about doing the same on the AGNews dataset? What causes the difference? • What are the differences between CBOW and Skipgram? • Create a document embedding, using max-pooling, for the first 10 documents in the AGNews dataset. • Which document is most similar to the first document? <br><br>document nr: 7 <br> source: data/week 6.pdf <br> provenance: 0 <br> JM2050 Natural Language Processing 2024 –2025 Uzay Kaymak 43 By default, the number of topics need not be specified to  BERTopic 43 Number of topics <br><br>document nr: 8 <br> source: data/Week 6 reading a - BERTopic-NeuralTopicModelingWithAClassBasedTF-IDFprocedure (1).pdf <br> provenance: 0 <br> tations, signiﬁcant ﬂexibility is introduced in the model allowing for ease of usability. We present in this paper an in-depth analysis of BERTopic, ranging from evaluation studies with classical topic coherence measures to analyses in- volving running times. Our experiments suggest that BERTopic learns coherent patterns of language and demonstrates competitive and stable perfor- mance across a variety of tasks. References Charu C Aggarwal, Alexander Hinneburg, and <br><br>document nr: 9 <br> source: data/Week 6 reading c 2023-IFSA-InterpretingTopicModelsWithChatGPT.pdf <br> provenance: 0 <br> ability method for bert-based model in fake news detection,” Scientific reports, vol. 11, no. 1, p. 23705, 2021. [42] V . W. Anelli, G. M. Biancofiore, A. De Bellis, T. Di Noia, and E. Di Sciascio, “Interpretability of bert latent space through knowledge graphs,” in Proceedings of the 31st ACM International Conference on Information & Knowledge Management , 2022, pp. 3806–3810. <br><br>document nr: 10 <br> source: data/Week 12 b deep_learning_3.pdf <br> provenance: 0 <br> Attention modelsAlthough BERT uses a different architecture because it does-Masked language modeling-Next sentence prediction <br><br>document nr: 11 <br> source: data/Week 6 reading a - BERTopic-NeuralTopicModelingWithAClassBasedTF-IDFprocedure (1).pdf <br> provenance: 0 <br> when using BERTopic and will be reﬂected in the experimental setup. 5 Experimental Setup OCTIS (Optimizing and Comparing Topic models is Simple), an open-source python package, was used to run the experiments, validate results, and preprocess the data (Terragni et al., 2021). Both the implementation of BERTopic as well as the experimental setup are freely available online. 12 5.1 Datasets Three datasets were used to validate BERTopic, namely 20 NewsGroups, BBC News, and Trump’s <br><br>document nr: 12 <br> source: data/Week 6 reading a - BERTopic-NeuralTopicModelingWithAClassBasedTF-IDFprocedure (1).pdf <br> provenance: 0 <br> BERTopic. Second, although BERTopic allows for a con- textual representation of documents through its transformer-based language models, the topic rep- resentation itself does not directly account for that as they are generated from bags-of-words. The words in a topic representation merely sketch the importance of words in a topic whilst those words are likely to be related. As a result, words in a topic might be similar to one another and can be redundant for the interpretation of the topic. In <br><br>document nr: 13 <br> source: data/Week 6 reading a - BERTopic-NeuralTopicModelingWithAClassBasedTF-IDFprocedure (1).pdf <br> provenance: 0 <br> modeling techniques can be used across a variety of use cases, many of which are not covered in this study. For those reasons, we additionally discuss the strengths and weaknesses of BERTopic to fur- ther describe when, and perhaps most importantly, when not to use BERTopic. 7.1 Strengths There are several notable strengths of BERTopic compared to the topic models used in this study. First, the experiments demonstrate that BERTopic remains competitive regardless of the <br><br>                         |
| **LLM's Final Answer** | According to Document 2, which discusses BERTopic-Neural Topic Modeling With A Class-Based TF-IDF procedure, there isn't an explicit discussion about the limitations of BERTopic. However, I can infer some potential limitations based on related research.<br><br>One possible limitation of BERTopic could be its reliance on pre-trained language models like SBERT or Doc2Vec, which might not perform well on low-resource languages or domains where these models haven't been extensively trained. This is because BERTopic leverages the strengths of these pre-trained models to generate topics.<br><br>Another potential limitation could be the computational cost associated with training BERTopic, especially when working with large datasets. As mentioned in Document 2, the authors used multiple language models and fixed parameters for comparison purposes, indicating that BERTopic may require significant computational resources.<br><br>Additionally, Document 1 discusses various topic modeling techniques, including those that don't rely on pre-trained language models. These alternatives might offer more flexibility and adaptability to specific domains or languages, potentially making them more suitable for certain applications.<br><br>Please note that these are inferred limitations and not explicitly stated in the provided documents. For a comprehensive understanding of BERTopic's limitations, it would be best to consult the original paper by Song et al. (2020) or other relevant sources.                          | According to Document 5, "BERTopic assumes that each document only contains a single topic" which is a limitation of the model. Additionally, Document 12 mentions that the topic representation itself does not directly account for contextual relationships between words, making the interpretation of topics potentially redundant.<br><br>Furthermore, Document 13 highlights that while BERTopic has several strengths, there are also weaknesses to consider. Specifically, it notes that the model assumes a single topic per document, which may not accurately capture complex or multi-topic documents.<br><br>Source: Documents 5, 12, and 13.                       |
| **Time Taken**         | 70.46 seconds                            | 13.46 seconds                         |

### Question 5: how do researchers propose attention models to work?What is the methodology of word2vec paper

| Field                  | With HYDE                                   | Without HYDE                                |
|------------------------|---------------------------------------------|---------------------------------------------|
| **Documents Found**    | document nr: 0 <br> source: data/Week 3 reading b - vectorsemantics2024.pdf <br> provenance: 0.991 <br> this context window is to the target word. The probability is based on applying the logistic (sigmoid) function to the dot product of the embeddings of the target word with each context word. To compute this probability, we just need embeddings for each target word and context word in the vocabulary. 1 W C aardvark zebra zebra aardvark apricot apricot \|V\| \|V\|+1 2V & = target words context & noise words … … 1..d … … Figure 6.13 The embeddings learned by the skipgram model. The algorithm stores two <br><br>document nr: 1 <br> source: data/Week 3 reading b - vectorsemantics2024.pdf <br> provenance: 0.989 <br> this context window is to the target word. The probability is based on applying the logistic (sigmoid) function to the dot product of the embeddings of the target word with each context word. We could thus compute this probability if only we had embeddings for each target word and context word in the vocabulary. Let’s now turn to learning these embeddings (which is the real goal of training this classiﬁer in the ﬁrst place). 6.8.2 Learning skip-gram embeddings <br><br>document nr: 2 <br> source: data/Week 3 reading b - vectorsemantics2024.pdf <br> provenance: 0.984 <br> Loss function for one w with cpos, cneg1...cnegkMaximize the similarity of the target with the actual context words, and minimize the similarity of the target with the k negative sampled non-neighbor words.  6.8 • W ORD 2 VEC 21 Given the set of positive and negative training instances, and an initial set of embed- dings, the goal of the learning algorithm is to adjust those embeddings to • Maximize the similarity of the target word, context word pairs ( w , c pos ) drawn from the positive examples <br><br>                            | document nr: 0 <br> source: data/Week 6 reading b -2021-SSCI-AComparativeStudyOfFuzzyTopicModelsAndLDAInTermsOfInterpretability.pdf <br> provenance: 0.959 <br> The contributions of this paper are: 1) we propose two new topic modeling algorithms, 2) we compare four different topic models in terms of interpretability, 3) we study the relation between various parameters of the topic models and their properties that can could be useful 978-1-7281-9048-8/21/$31.00 ©2021 IEEE <br><br>document nr: 1 <br> source: data/Week 10deep_learning_2.pdf <br> provenance: 0.91 <br> Attention modelsThis is what attention does! <br><br>document nr: 2 <br> source: data/Week 6 reading d - LatenDirichletAllocation (1).pdf <br> provenance: 0.712 <br> IR researchers for text corpora—a methodology successfully deployed in modern Internet search engines—reduces each document in the corpus to a vector of real numbers, each of which repre- sents ratios of counts. In the populartf-idf scheme (Salton and McGill, 1983), a basic vocabulary of “words” or “terms” is chosen, and, for each document in the corpus, a count is formed of the number of occurrences of each word. After suitable normalization, this term frequency count is <br><br>document nr: 3 <br> source: data/Week 12 b deep_learning_3.pdf <br> provenance: 0 <br> Attention modelsWhat about text classification?Well maybe with some prompt enineeringSpeaking of which… <br><br>document nr: 4 <br> source: data/Week 6 reading d - LatenDirichletAllocation (1).pdf <br> provenance: 0 <br> data. The goal is to ﬁnd short descriptions of the members of a collection that enable efﬁcient processing of large collections while preserving the essential statistical relationships that are useful for basic tasks such as classiﬁcation, novelty detection, summarization, and similarity and relevance judgments. Signiﬁcant progress has been made on this problem by researchers in the ﬁeld of informa- tion retrieval (IR) (Baeza-Y ates and Ribeiro-Neto, 1999). The basic methodology proposed by <br><br>document nr: 5 <br> source: data/Week 10deep_learning_2.pdf <br> provenance: 0 <br> Attention modelsSo comparing to word2vec, consider thisThe thief was robbing a bankOn our safari, we saw many crocodiles on the bank of the riverword2vec will give us the same vector for bank in both sentencesbut using attention will give us different vectors! <br><br>document nr: 6 <br> source: data/Week 9 reading.pdf <br> provenance: 0 <br> convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring signiﬁcantly <br><br>document nr: 7 <br> source: data/Week 12 b deep_learning_3.pdf <br> provenance: 0 <br> Attention modelsGPT1GPT2This made reasonably well-written documents when used wellGPT3WuDAOBLOOMChatGPT / GPT 4 <br><br>document nr: 8 <br> source: data/Week 10deep_learning_2.pdf <br> provenance: 0 <br> Attention modelsSome key observations:1.We need to pay attention to multiple words, from multiple angles, hence the multi-head <br><br>document nr: 9 <br> source: data/Week 10deep_learning_2.pdf <br> provenance: 0 <br> Attention modelsAttention models three thingsK – KeyV – ValueQ – QueryThe word of interest that we are looking into(in seq2seq: the output word) <br><br>document nr: 10 <br> source: data/Week 12 b deep_learning_3.pdf <br> provenance: 0 <br> Attention modelsThen there are a bunch focused on non-NLP tasks <br><br>document nr: 11 <br> source: data/Week 12 b deep_learning_3.pdf <br> provenance: 0 <br> Attention modelsExample for topic modelling <br><br>document nr: 12 <br> source: data/Week 12 b deep_learning_3.pdf <br> provenance: 0 <br> Attention modelsThere is a competitor named XLM (cross lingual model) <br><br>document nr: 13 <br> source: data/Week 12 b deep_learning_3.pdf <br> provenance: 0 <br> Attention modelsAnd then there is text generation! <br><br>                         |
| **LLM's Final Answer** | According to the provided documents, I found information related to the Word2Vec paper and its methodology.<br><br>In the Word2Vec paper, researchers propose a method called "skip-gram" to learn word embeddings. This involves computing the probability of a target word given its context words within a certain window size (as shown in Document 0 and Document 1).<br><br>To achieve this, they apply the logistic (sigmoid) function to the dot product of the embeddings of the target word with each context word. However, they don't directly provide the embeddings for each target word and context word; instead, they aim to learn these embeddings through the training process.<br><br>The learning algorithm (described in Document 2) maximizes the similarity of the target word with the actual context words and minimizes the similarity with the negative sampled non-neighbor words. This is done by adjusting the embeddings to optimize the loss function.<br><br>The key steps of the Word2Vec methodology involve:<br><br>1. Computing the probability of a target word given its context words using the logistic function and dot products of embeddings.<br>2. Learning the embeddings through maximizing the similarity of target-context word pairs and minimizing the similarity with negative samples.<br><br>Source: Documents 0, 1, and 2 from "data/Week 3 reading b - vectorsemantics2024.pdf".                          | To answer your question, I'll refer to Document 5, which discusses the comparison of attention models to word2vec.<br><br>According to Document 5, "Attention models" (source: data/Week 10 deep_learning_2.pdf), researchers propose attention models to work by focusing on specific parts of the input sequence that are most relevant to the task at hand. This is in contrast to traditional methods like word2vec, which treats all words equally and assigns them the same weight.<br><br>Regarding the methodology of the word2vec paper, it is not explicitly mentioned in the provided documents. However, based on general knowledge, the word2vec paper proposes a method for representing words as vectors in a high-dimensional space, where semantically similar words are mapped to nearby points. The methodology involves training a neural network to predict the context words given a target word, and the weights learned during this process are used to compute the vector representation of each word.<br><br>For more information on the word2vec paper, please consult the original research paper by Mikolov et al. (2013).<br><br>Source: Document 5, "Attention models", source: data/Week 10 deep_learning_2.pdf                       |
| **Time Taken**         | 46.5 seconds                            | 19.25 seconds                         |

### Question 6: What model is faster than LDA?

| Field                  | With HYDE                                   | Without HYDE                                |
|------------------------|---------------------------------------------|---------------------------------------------|
| **Documents Found**    | document nr: 0 <br> source: data/Week 3 reading a - nb24aug.pdf <br> provenance: 0.49 <br> Text Classification and Naive BayesSentiment and Binary Naive Bayes <br><br>document nr: 1 <br> source: data/Week 3 reading a - nb24aug.pdf <br> provenance: 0.423 <br> Text Classification and Naive BayesThe Naive Bayes Classifier <br><br>                            | document nr: 0 <br> source: data/Week 2 reading - 2_TextProc_2023.pdf <br> provenance: 0.966 <br> Capture groups: multiple registers/the (.*)er they (.*), the \1er we \2/ Matchesthe faster they ran, the faster we ranBut notthe faster they ran, the faster we ate <br><br>document nr: 1 <br> source: data/Week 9 reading.pdf <br> provenance: 0.961 <br> in Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0 BLEU, establishing a new state-of-the-art BLEU score of 28.4. The conﬁguration of this model is listed in the bottom line of Table 3. Training took 3.5 days on 8 P100 GPUs. Even our base model surpasses all previously published models and ensembles, at a fraction of the training cost of any of the competitive models. <br><br>document nr: 2 <br> source: data/Week 2.pdf <br> provenance: 0.904 <br> JM2050 Natural Language Processing 2024 –2025 Uzay Kaymak 50 www.jads.nl Capture groups: multiple registers the (.*)er they (.*), the \\1er we \\2  Matches the faster they ran, the faster we ran But not the faster they ran, the faster we ate <br><br>document nr: 3 <br> source: data/Week 6 reading d - LatenDirichletAllocation (1).pdf <br> provenance: 0 <br> deed, as we discuss in Section 5, we adopt the empirical Bayes approach to estimating parameters such asα and β in simple implementations of LDA, but we also consider fuller Bayesian approaches as well. 997 <br><br>document nr: 4 <br> source: data/Week 9 reading.pdf <br> provenance: 0 <br> different layer types. As noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially executed operations, whereas a recurrent layer requires O(n) sequential operations. In terms of computational complexity, self-attention layers are faster than recurrent layers when the sequence length n is smaller than the representation dimensionality d, which is most often the case with <br><br>document nr: 5 <br> source: data/Week 3 reading c - Mykolo etal (2013)-Word2Vec.pdf <br> provenance: 0 <br> previously shown that they perform signiﬁcantly better than LSA for preserving linear regularities among words [20, 31]; LDA moreover becomes computationally very expensive on large data sets. Similar to [18], to compare different model architectures we deﬁne ﬁrst the computational complex- ity of a model as the number of parameters that need to be accessed to fully train the model. Next, we will try to maximize the accuracy, while minimizing the computational complexity. <br><br>document nr: 6 <br> source: data/Week 6 reading a - BERTopic-NeuralTopicModelingWithAClassBasedTF-IDFprocedure (1).pdf <br> provenance: 0 <br> Figure 1: Computation time (wall time) in seconds of each topic model on the Trump dataset. Increasing sizes of vocabularies were regulated through selection of documents ranging from 1000 documents until 43000 documents with steps of 2000. Left: computational results with CTM. Right: computational results without CTM as it inﬂates the y-axis making differentiation between other topic models difﬁcult to visualize. and LDA, are faster than the neural network-based <br><br>document nr: 7 <br> source: data/Week 6 reading a - BERTopic-NeuralTopicModelingWithAClassBasedTF-IDFprocedure (1).pdf <br> provenance: 0 <br> NET SBERT model, is quite slow compared to all other models. If we remove that model from the re- sults, we can more easily compare the wall time of the topic models that are more close in speed. Then, We can observe that the classical models, NMF <br><br>document nr: 8 <br> source: data/Week 9 deep_learning_1.pdf <br> provenance: 0 <br> GloVeHey! This is something that we do in LSA/LDA too! <br><br>document nr: 9 <br> source: data/Week 7 reading .pdf <br> provenance: 0 <br> models at this scale. 16 <br><br>document nr: 10 <br> source: data/Week 6 reading d - LatenDirichletAllocation (1).pdf <br> provenance: 0 <br> representation provided by LDA may be useful as a fast ﬁltering algorithm for feature selection in text classiﬁcation. 1013 <br><br>document nr: 11 <br> source: data/Week 6 reading d - LatenDirichletAllocation (1).pdf <br> provenance: 0 <br> particular the LDA model shown in Figure 5 (left). The problematic coupling between θ and β 1003 <br><br>document nr: 12 <br> source: data/Week 6 reading d - LatenDirichletAllocation (1).pdf <br> provenance: 0 <br> 250 300 350 400 450 500 550 600 Number of Topics Predictive Perplexity LDA Fold in pLSI Smoothed Mixt. Unigrams Figure 11: Results for collaborative ﬁltering on the EachMovie data. Figure 10 shows our results. We see that there is little reduction in classiﬁcation performance in using the LDA-based features; indeed, in almost all cases the performance is improved with the LDA features. Although these results need further substantiation, they suggest that the topic-based <br><br>document nr: 13 <br> source: data/Week 6 reading d - LatenDirichletAllocation (1).pdf <br> provenance: 0 <br> tional bounds. It is also possible to achieve higher accuracy by dispensing with the requirement of maintaining a bound, and indeed Minka and Lafferty (2002) have shown that improved inferential accuracy can be obtained for the LDA model via a higher-order variational technique known as ex- pectation propagation. Finally, Grifﬁths and Steyvers (2002) have presented a Markov chain Monte Carlo algorithm for LDA. LDA is a simple model, and although we view it as a competitor to methods such as LSI and <br><br>                         |
| **LLM's Final Answer** | To determine which model is faster than Latent Dirichlet Allocation (LDA), I'll refer to Document 1: "Text Classification and Naive Bayes" (Source: data/Week 3 reading a - nb24aug.pdf).<br><br>According to this document, Naive Bayes is generally faster than LDA because it has fewer parameters to estimate and doesn't require complex computations like LDA does.<br><br>In fact, the document states that Naive Bayes is often preferred when speed is crucial in text classification tasks, especially for large datasets. <br><br>Therefore, based on this information, Naive Bayes is indeed faster than LDA.                          | According to Document 4, "self-attention layers are faster than recurrent layers when the sequence length n is smaller than the representation dimensionality d." However, I did not find information about a model being faster than LDA directly.<br><br>However, according to Document 1, the author mentions that their model outperforms the best previously reported models, including ensembles, and surpasses all previously published models and ensembles, at a fraction of the training cost of any of the competitive models. But it does not explicitly state that it is faster than LDA.<br><br>On the other hand, Document 6 states that "CTM and LDA, are faster than the neural network-based" model.                       |
| **Time Taken**         | 35.83 seconds                            | 14.71 seconds                         |

### Question 7: What is the fastest topic modeling system?

| Field                  | With HYDE                                   | Without HYDE                                |
|------------------------|---------------------------------------------|---------------------------------------------|
| **Documents Found**    | document nr: 0 <br> source: data/week 6.pdf <br> provenance: 0.992 <br> structure of the documents and extract topics (e.g. BERTopic). 1) Blei, D. M., Ng, A. Y., & Jordan, M. I. (2003). Latent dirichlet allocation. Journal of machine Learning research, 3(Jan), 993-1022. 2) Grootendorst, M. (2022). BERTopic: Neural topic modeling with a class-based TF-IDF procedure. arXiv preprint arXiv:2203.05794. 3) Landauer, T. K., Foltz, P. W., & Laham, D. (1998). An introduction to latent semantic analysis. Discourse processes, 25(2-3), 259-284. <br><br>document nr: 1 <br> source: data/Week 6 reading b -2021-SSCI-AComparativeStudyOfFuzzyTopicModelsAndLDAInTermsOfInterpretability.pdf <br> provenance: 0.988 <br> [3] Blei, D. M., Ng, A. Y ., & Jordan, M. I. (2003). Latent dirichlet allocation. the Journal of machine Learning research, 3, 993-1022. [4] Blei, D. M., & Lafferty, J. D. (2006). Dynamic topic models. In Proceedings of the 23rd international conference on Machine learning (pp. 113-120). [5] Blei, D., & Lafferty, J. (2006). Correlated topic models. Advances in neural information processing systems, 18, 147. [6] Blei, D. M. (2012). Probabilistic topic models. Communications of the ACM, 55(4), 77-84. <br><br>document nr: 2 <br> source: data/Week 6 reading b -2021-SSCI-AComparativeStudyOfFuzzyTopicModelsAndLDAInTermsOfInterpretability.pdf <br> provenance: 0.958 <br> based topic model [37]. For documents with more words, the Topics over Time model [48] and Dynamic Topic Model [4] are suitable for capturing changes of topics over time. The Pachinko Allocation Model [22] and the Correlated Topic Model [5] are suitable when there is an interest in the correlation between topics. The most popular topic modelling technique is called Latent Dirichlet Allocation (LDA) [3]. LDA performs well on data documents with more than 50 words and no complex topic <br><br>document nr: 3 <br> source: data/Week 6 reading b -2021-SSCI-AComparativeStudyOfFuzzyTopicModelsAndLDAInTermsOfInterpretability.pdf <br> provenance: 0.87 <br> indicating the most important topics for its decisions. A problem with current topic models is they do not always give interpretable results. Fuzzy Latent Semantic Analysis (FLSA) [19] is a topic modeling algorithm applied to medical data and showed superior performance to the most popular model Latent Dirichlet Allocation (LDA). FLSA is based on grouping similar documents in terms of the words they contain. The membership of each document to a topic is <br><br>                            | document nr: 0 <br> source: data/week 6.pdf <br> provenance: 0.937 <br> JM2050 Natural Language Processing 2024 –2025 Uzay Kaymak 6 What are topic models Topic model = an (unsupervised) statistical model to discover latent  topics from a corpus. 6 Input: corpus + number of topics Output: Two matrices: 1. P(word\|topic) (words x topics) 2. P(topic\|document)    (topics x documents) Per topic, the top nwords are typically used to  represent a topic Traditional topic modeling  algorithms return two matrices. This need not be the case for all models. <br><br>document nr: 1 <br> source: data/Week 8 Tutorial Topic Modeling.pdf <br> provenance: 0.934 <br> BERTopic For the first ten documents in topic_assigned_to_doc,  compare the produced topic to the document. Do you agree this  document reflects the topic well? What do you think -1 means? How many words per topic do you have? What is the effect on the  topic quality if you work with more topics? <br><br>document nr: 2 <br> source: data/Week 6 reading b -2021-SSCI-AComparativeStudyOfFuzzyTopicModelsAndLDAInTermsOfInterpretability.pdf <br> provenance: 0.879 <br> The ﬁrst step in developing a topic-based text classiﬁcation algorithm is selecting a suitable topic model. A good topic model should both be interpretable and leading to high classiﬁ- cation performance. Typically, classiﬁcation models use many (at least 50) topics [19], [40]. Therefore, topic models must remain coherent as the number of topics increase. Many different topic models exist, all having a speciﬁc purpose [47]. Applicable topic models for documents with less <br><br>document nr: 3 <br> source: data/Week 6 reading c 2023-IFSA-InterpretingTopicModelsWithChatGPT.pdf <br> provenance: 0 <br> deterministic algorithms. Although there is growing literature about LLM explainbility [40]–[42], there is no guarantee that the produced output is truthful. For this reason, the topic explanations provided by ChatGPT should be used to support topic modeling interpretation, but should not be seen as an objectively truthful answer. VI. C ONCLUSION Summarizing, this paper explores a novel approach for interpreting topic modeling output by leveraging the large <br><br>document nr: 4 <br> source: data/Week 6 reading a - BERTopic-NeuralTopicModelingWithAClassBasedTF-IDFprocedure (1).pdf <br> provenance: 0 <br> Figure 1: Computation time (wall time) in seconds of each topic model on the Trump dataset. Increasing sizes of vocabularies were regulated through selection of documents ranging from 1000 documents until 43000 documents with steps of 2000. Left: computational results with CTM. Right: computational results without CTM as it inﬂates the y-axis making differentiation between other topic models difﬁcult to visualize. and LDA, are faster than the neural network-based <br><br>document nr: 5 <br> source: data/Week 9 deep_learning_1.pdf <br> provenance: 0 <br> Auto-encodersBut you can also do topic modeling with this <br><br>document nr: 6 <br> source: data/Week 6 reading c 2023-IFSA-InterpretingTopicModelsWithChatGPT.pdf <br> provenance: 0 <br> given a document, respectively. Then, the top-N words with the highest propensity per topic are typically taken to represent a topic. The most popular topic modeling algorithm is LDA [12]. It assumes that each document in a corpus is a mixture of topics and is a probability distribution over words. LDA iteratively assigns words to topics and topics to documents, optimizing the likelihood of the observed data. FLSA-W [13] is a fuzzy-clustering-based topic modeling algorithm. It starts <br><br>document nr: 7 <br> source: data/Week 6 reading a - BERTopic-NeuralTopicModelingWithAClassBasedTF-IDFprocedure (1).pdf <br> provenance: 0 <br> allow for fast computation. Moreover, this method can also be used to model topic representations by other meta-data, such as author or journal. 4.1 Smoothing Although we can observe how topic representa- tions are different from one time to another, the topic representation at timestep t is independent of timestep t-1. As a result, this dynamic representa- tion of topics might not result in linearly evolving topics. When we expect linearly evolving topics, <br><br>document nr: 8 <br> source: data/Week 6 reading a - BERTopic-NeuralTopicModelingWithAClassBasedTF-IDFprocedure (1).pdf <br> provenance: 0 <br> 2021). In part, the authors suggest a needs-driven approach to evaluation as topic modeling’s primary use is in computer-assisted content analysis. To this purpose, the differences in running times of each model were explored as they can greatly impact their usability. Here, we choose to focus on the wall times as it more accurately reﬂects how the topic modeling techniques would be used in practice. All the models are run on a machine with 2 cores of Intel(R) Xeon(R) CPU @ 2.00GHz and <br><br>document nr: 9 <br> source: data/Week 12 b deep_learning_3.pdf <br> provenance: 0 <br> Attention modelsExample for topic modelling <br><br>document nr: 10 <br> source: data/Week 6 reading a - BERTopic-NeuralTopicModelingWithAClassBasedTF-IDFprocedure (1).pdf <br> provenance: 0 <br> (Cer et al., 2018), Doc2Vec, and the "all-MiniLM- L6-v2" (MiniLM) and "all-mpnet-base-v2" (MP- NET) SBERT models. Finally, BERTopic, with and without the assump- tion of linearly-evolving topics, was compared with the original dynamic topic model, referred hereto as LDA Sequence. 5.3 Evaluation The performance of the topic models in this paper is reﬂected by two widely-used metrics, namely topic coherence and topic diversity. For each topic model, its topic coherence was evaluated using <br><br>document nr: 11 <br> source: data/week 6.pdf <br> provenance: 0 <br> JM2050 Natural Language Processing 2024 –2025 Uzay Kaymak 26 Factorization-based topic models <br><br>document nr: 12 <br> source: data/Week 6 reading a - BERTopic-NeuralTopicModelingWithAClassBasedTF-IDFprocedure (1).pdf <br> provenance: 0 <br> 2020a. Pre-training is a hot topic: Contextual- ized document embeddings improve topic coher- ence. arXiv preprint arXiv:2004.03974. Federico Bianchi, Silvia Terragni, Dirk Hovy, Debora Nozza, and Elisabetta Fersini. 2020b. Cross-lingual contextualized topic models with zero-shot learning. arXiv preprint arXiv:2004.07737. David M Blei and John D Lafferty. 2006. Dynamic topic models. In Proceedings of the 23rd interna- tional conference on Machine learning , pages 113– 120. <br><br>                         |
| **LLM's Final Answer** | Based on the provided documents, I found that the Latent Dirichlet Allocation (LDA) [3] is mentioned as performing well on data documents with more than 50 words and no complex topic structures (Document 2). However, it does not provide information about the speed of the system.<br><br>To find the answer, I also looked at Document 1, which mentions several topic modeling techniques, including Latent Dirichlet Allocation (LDA) [3], Dynamic Topic Model [4], and Correlated Topic Model [5]. Unfortunately, none of these documents explicitly state the speed of these systems.<br><br>However, I did find some related information in Document 2, where it states that "For documents with more words, the Topics over Time model [48] and Dynamic Topic Model [4] are suitable for capturing changes of topics over time." This implies that the Dynamic Topic Model might be faster or more efficient for large datasets due to its ability to capture temporal changes.<br><br>Unfortunately, without explicit information about the speed of these systems, I cannot definitively say which one is the fastest. If you need further clarification or details about the speed of these topic modeling systems, please let me know, and I'll try to help you find the necessary information.                          | According to Document 4, which compares the computation time (wall time) of various topic models on the Trump dataset, we can see that some topic models like CTM (Computationally Efficient Topic Model) are faster than others.<br><br>However, based on the information provided, I cannot definitively say which topic modeling system is the absolute fastest. But, according to the graph shown in Document 4, CTM seems to perform better in terms of speed when dealing with larger vocabularies.<br><br>Source: Document 4, id: 358113e00fc16c479a0cdc8511784590, pk: 454311103152261756, source: data/Week 6 reading a - BERTopic-NeuralTopicModelingWithAClassBasedTF-IDFprocedure (1).pdf                       |
| **Time Taken**         | 40.54 seconds                            | 16.05 seconds                         |

### Question 8: Word representations across different languages using SkipGram

| Field                  | With HYDE                                   | Without HYDE                                |
|------------------------|---------------------------------------------|---------------------------------------------|
| **Documents Found**    | document nr: 0 <br> source: data/Week 6 reading a - BERTopic-NeuralTopicModelingWithAClassBasedTF-IDFprocedure (1).pdf <br> provenance: 0.997 <br> 2020). CTM, for example, demonstrates the ad- vantage of relying on pre-trained language models, namely that future improvements in language mod- els may translate into better topic models (Bianchi et al., 2020a). Several approaches have started simplifying the topic building process by clustering word- and document embeddings (Sia et al., 2020; Angelov, 2020). This clustering approach allows for a ﬂex- ible topic model as the generation of the clusters can be separated from the process of generating the <br><br>document nr: 1 <br> source: data/Week 6 reading a - BERTopic-NeuralTopicModelingWithAClassBasedTF-IDFprocedure (1).pdf <br> provenance: 0.991 <br> convert sentences and paragraphs to dense vector representations using pre-trained language models. It achieves state-of-the-art performance on various sentence embedding tasks (Reimers and Gurevych, 2020; Thakur et al., 2020). These embeddings, however, are primarily used to cluster semantically similar documents and not directly used in generating the topics. Any other embedding technique can be used for this purpose if the language model generating the document em- <br><br>document nr: 2 <br> source: data/Week 8 reading.pdf <br> provenance: 0.959 <br> sample the size cbetween 1 and 5. In order to sub- sample the most frequent words, we use a rejection threshold of 10−4 (for more details, see (Mikolov et al., 2013b)). When building the word dictionary, we keep the words that appear at least 5 times in the training set. The step size γ0 is set to 0.025 for the skipgram baseline and to 0.05 for both our model and the cbow baseline. These are the default values in the word2vec package and work well for our model too. <br><br>document nr: 3 <br> source: data/Week 3 reading c - Mykolo etal (2013)-Word2Vec.pdf <br> provenance: 0 <br> 6 Conclusion In this paper we studied the quality of vector representations of words derived by various models on a collection of syntactic and semantic language tasks. We observed that it is possible to train high quality word vectors using very simple model architectures, compared to the popular neural network models (both feedforward and recurrent). Because of the much lower computational complexity, it is possible to compute very accurate high dimensional word vectors from a much larger data set. <br><br>                            | document nr: 0 <br> source: data/Week 8 reading.pdf <br> provenance: 0.973 <br> Enriching Word Vectors with Subword Information Piotr Bojanowski∗and Edouard Grave∗and Armand Joulin and Tomas Mikolov Facebook AI Research {bojanowski,egrave,ajoulin,tmikolov}@fb.com Abstract Continuous word representations, trained on large unlabeled corpora are useful for many natural language processing tasks. Popular models that learn such representations ignore the morphology of words, by assigning a dis- tinct vector to each word. This is a limitation, especially for languages with large vocabular- <br><br>document nr: 1 <br> source: data/Week 8 reading.pdf <br> provenance: 0.947 <br> forms, while the Finnish language has ﬁfteen cases for nouns. These languages contain many word forms that occur rarely (or not at all) in the training corpus, making it difﬁcult to learn good word rep- resentations. Because many word formations follow rules, it is possible to improve vector representations for morphologically rich languages by using charac- ter level information. In this paper, we propose to learn representations for character n-grams, and to represent words as the <br><br>document nr: 2 <br> source: data/Week 8 reading.pdf <br> provenance: 0.929 <br> subword information. Our approach, which incor- porates character n-grams into the skipgram model, is related to an idea that was introduced by Schütze (1993). Because of its simplicity, our model trains fast and does not require any preprocessing or super- vision. We show that our model outperforms base- lines that do not take into account subword informa- tion, as well as methods relying on morphological analysis. We will open source the implementation <br><br>document nr: 3 <br> source: data/Week 4 tutorial.pdf <br> provenance: 0 <br> Word2Vec: Continuous Bag of Words • Task: Given a context, predict the word This is a visual comparison https://kavita-ganesan.com/comparison-between- cbow-skipgram-subword/ <br><br>document nr: 4 <br> source: data/Week 6 reading a - BERTopic-NeuralTopicModelingWithAClassBasedTF-IDFprocedure (1).pdf <br> provenance: 0 <br> time and the extent to which topic representations reﬂect that. In BERTopic, we can model this behavior by leveraging the c-TF-IDF representations of topics. Here, we assume that the temporal nature of topics should not inﬂuence the creation of global topics. The same topic might appear across different times, albeit possibly represented differently. As an exam- ple, a global topic about cars might contain words such as "car" and "vehicle" regardless of the tem- <br><br>document nr: 5 <br> source: data/Week 3.pdf <br> provenance: 0 <br> JM2050 Natural Language Processing 2024 – 2025 Uzay Kaymak 53 www.jads.nl The Bag of Words Representation <br><br>document nr: 6 <br> source: data/Week 8 reading.pdf <br> provenance: 0 <br> similar representations. Soricut and Och (2015) described a method to learn vector representations of morphological transformations, allowing to ob- tain representations for unseen words by applying these rules. Word representations trained on mor- phologically annotated data were introduced by Cot- terell and Schütze (2015). Closest to our approach, Schütze (1993) learned representations of character four-grams through singular value decomposition, and derived representations for words by summing <br><br>document nr: 7 <br> source: data/Week 3 reading b - vectorsemantics2024.pdf <br> provenance: 0 <br> embeddings for each target word and context word in the vocabulary. Let’s now turn to learning these embeddings (which is the real goal of training this classiﬁer in the ﬁrst place). 6.8.2 Learning skip-gram embeddings Word2vec learns embeddings by starting with an initial set of embedding vectors and then iteratively shifting the embedding of each word w to be more like the em- beddings of words that occur nearby in texts, and less like the embeddings of words <br><br>document nr: 8 <br> source: data/Week 3 reading c - Mykolo etal (2013)-Word2Vec.pdf <br> provenance: 0 <br> 1.2 Previous Work Representation of words as continuous vectors has a long history [10, 26, 8]. A very popular model architecture for estimating neural network language model (NNLM) was proposed in [1], where a feedforward neural network with a linear projection layer and a non-linear hidden layer was used to learn jointly the word vector representation and a statistical language model. This work has been followed by many others. <br><br>document nr: 9 <br> source: data/Week 3 reading b - vectorsemantics2024.pdf <br> provenance: 0 <br> jam apricot c pos matrix Tolstoy move apricot  and Tolstoy  apart decreasing c neg2  z w ! c neg1 c neg2 k=2 Figure 6.14 Intuition of one step of gradient descent. The skip-gram model tries to shift embeddings so the target embeddings (here for apricot ) are closer to (have a higher dot prod- uct with) context embeddings for nearby words (here jam ) and further from (lower dot product with) context embeddings for noise words that don’t occur nearby (here Tolstoy and matrix ). <br><br>document nr: 10 <br> source: data/Week 8 reading.pdf <br> provenance: 0 <br> ies and many rare words. In this paper, we pro- pose a new approach based on the skipgram model, where each word is represented as a bag of character n-grams. A vector represen- tation is associated to each character n-gram; words being represented as the sum of these representations. Our method is fast, allow- ing to train models on large corpora quickly and allows us to compute word representations for words that did not appear in the training data. We evaluate our word representations on <br><br>document nr: 11 <br> source: data/Week 8 reading.pdf <br> provenance: 0 <br> model too. Using this setting on English data, our model with character n-grams is approximately 1.5× slower to train than the skipgram baseline. Indeed, we process 105k words/second/thread versus 145k words/second/thread for the baseline. Our model is implemented in C++, and is publicly available.3 4.4 Datasets Except for the comparison to previous work (Sec. 5.3), we train our models on Wikipedia data.4 We downloaded Wikipedia dumps in nine languages: Arabic, Czech, German, English, <br><br>document nr: 12 <br> source: data/Week 3 reading b - vectorsemantics2024.pdf <br> provenance: 0 <br> t neg  h [ s ( c t neg · w )] w (6.39) w t + 1 = w t  h [ s ( c pos · w t )  1 ] c pos + k X i = 1 [ s ( c n eg i · w t )] c neg i (6.40) Just as in logistic regression, then, the learning algorithm starts with randomly ini- tialized W and C matrices, and then walks through the training corpus using gradient descent to move W and C so as to maximize the objective in Eq. 6.34 by making the updates in (Eq. 6.39 )-(Eq. 6.40 ). Recall that the skip-gram model learns two separate embeddings for each word i : <br><br>                         |
| **LLM's Final Answer** | According to Document 2, the Skip-Gram model is mentioned as one of the baselines used in the study, where the step size γ0 is set to 0.025 for the skipgram baseline.<br><br>Source: Document 2, id: 99162db67c15bc1fdc65739f7877514b                          | To address the question regarding word representations across different languages using SkipGram, I'll rely on Document 1 and Document 10.<br><br>According to Document 1, titled "Enriching Word Vectors with Subword Information," the authors propose a method to improve vector representations for morphologically rich languages by incorporating character-level information. They suggest representing words as the sum of their constituent character n-grams, which helps capture the morphology of words.<br><br>Similarly, Document 10 discusses a new approach based on the SkipGram model, where each word is represented as a bag of character n-grams. This method enables computing word representations for words that didn't appear in the training data and allows for faster training on large corpora.<br><br>Both documents highlight the importance of considering subword information when learning word representations, especially for languages with complex morphology. By doing so, the models can better capture the relationships between words and their components, leading to improved performance in various NLP tasks.<br><br>Source: Documents 1 and 10, specifically "Enriching Word Vectors with Subword Information" and "A New Approach Based on the Skipgram Model."                       |
| **Time Taken**         | 36.7 seconds                            | 17.87 seconds                         |

### Question 9: How did the researchers use attention in transformers?

| Field                  | With HYDE                                   | Without HYDE                                |
|------------------------|---------------------------------------------|---------------------------------------------|
| **Documents Found**    | document nr: 0 <br> source: data/Week 9 reading.pdf <br> provenance: 0.965 <br> position in the decoder to attend over all positions in the input sequence. This mimics the typical encoder-decoder attention mechanisms in sequence-to-sequence models such as [38, 2, 9]. • The encoder contains self-attention layers. In a self-attention layer all of the keys, values and queries come from the same place, in this case, the output of the previous layer in the encoder. Each position in the encoder can attend to all positions in the previous layer of the encoder. <br><br>document nr: 1 <br> source: data/Week 9 reading.pdf <br> provenance: 0.859 <br> query with all keys, divide each by √dk, and apply a softmax function to obtain the weights on the values. In practice, we compute the attention function on a set of queries simultaneously, packed together into a matrix Q. The keys and values are also packed together into matrices Kand V. We compute the matrix of outputs as: Attention(Q,K,V ) = softmax(QKT √dk )V (1) The two most commonly used attention functions are additive attention [2], and dot-product (multi- <br><br>document nr: 2 <br> source: data/Week 9 reading.pdf <br> provenance: 0.8 <br> textual entailment and learning task-independent sentence representations [4, 27, 28, 22]. End-to-end memory networks are based on a recurrent attention mechanism instead of sequence- aligned recurrence and have been shown to perform well on simple-language question answering and language modeling tasks [34]. To the best of our knowledge, however, the Transformer is the ﬁrst transduction model relying entirely on self-attention to compute representations of its input and output without using sequence- <br><br>document nr: 3 <br> source: data/Week 9 reading.pdf <br> provenance: 0 <br> 3.2 Attention An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key. 3 <br><br>                            | document nr: 0 <br> source: data/Week 8 Tutorial Topic Modeling.pdf <br> provenance: 0.964 <br> For FLSA-W and LDA Train topic models with both algorithms. • Use an iterative process in which you: • train a topic model, • remove useless words from the corpus , • retrain your model. • How many iterations did it take to find your topics? • What is the effect on the coherence score? A few considerations: • Setting the number of words per topics: Which number seems most intuitive? • Setting the number of topics: How did you select this number? <br><br>document nr: 1 <br> source: data/Week 12 b deep_learning_3.pdf <br> provenance: 0.919 <br> Attention modelsSo what can we use these Transformers for? <br><br>document nr: 2 <br> source: data/Week 7.pdf <br> provenance: 0.781 <br> JM2050 Natural Language Processing 2024 – 2025 U. Kaymak 5 What is Reasoning? Reasoning: the process of thinking about things in a logical way; opinions  and ideas that are based on logical thinking (Oxford Learner’s Dictionary) 5 Does the ant example demonstrate reasoning skills? How do we know the  model did not just learned this problem from the web? How can one test  reasoning skills? <br><br>document nr: 3 <br> source: data/Week 10deep_learning_2.pdf <br> provenance: 0 <br> Attention modelsThis is the transformers architectureLet’s break this down So then finallyWhat do these bits do?! <br><br>document nr: 4 <br> source: data/Week 7.pdf <br> provenance: 0 <br> JM2050 Natural Language Processing 2024 – 2025 U. Kaymak 9 How did we get here? 9 <br><br>document nr: 5 <br> source: data/Week 10deep_learning_2.pdf <br> provenance: 0 <br> Attention modelsThis paper led to a family of deep learning networks called “Transformers” <br><br>document nr: 6 <br> source: data/Week 4 reading b - 24_Dialogue_May_6_2021.pdf <br> provenance: 0 <br> Participant evaluationHuman chats with model for 6 turns and rates 8 dimensions of quality:•avoiding repetition, interestingness, making sense, fluency, listening, inquisitiveness, humanness, engagingness, (1) Avoiding Repetition: How repetitive was this user? ◦•Repeated themselves over and over •Sometimes said the same thing twice • Always said something new (3) Making sense: How often did this user say something which didn't make sense <br><br>document nr: 7 <br> source: data/Week 10deep_learning_2.pdf <br> provenance: 0 <br> Attention modelsThis is the transformers architectureLet’s break this down Those are attention <br><br>document nr: 8 <br> source: data/Week 12 b deep_learning_3.pdf <br> provenance: 0 <br> Attention modelsBecause what do we do with Transformers?Well… we take these pretrained models and finetune them! <br><br>document nr: 9 <br> source: data/Week 10deep_learning_2.pdf <br> provenance: 0 <br> Attention modelsSo what are these transformers and their attention? <br><br>document nr: 10 <br> source: data/Week 12 b deep_learning_3.pdf <br> provenance: 0 <br> Attention modelsBack to TransformersSo how do weuse this in practice? <br><br>document nr: 11 <br> source: data/Week 10deep_learning_2.pdf <br> provenance: 0 <br> Attention modelsAnd pretty much all of them can be readily found in Huggingface’s TransformersPython libraryhttps://huggingface.co/ <br><br>document nr: 12 <br> source: data/Week 10deep_learning_2.pdf <br> provenance: 0 <br> Attention modelsThis is the transformers architectureLet’s break this down And we add “positional” encoding to those inputs <br><br>document nr: 13 <br> source: data/Week 10deep_learning_2.pdf <br> provenance: 0 <br> Attention modelsThis is the transformers architectureLet’s break this down We’ve got outputs <br><br>                         |
| **LLM's Final Answer** | According to Document 1, researchers used the attention function in transformers by computing it on a set of queries simultaneously, packed together into a matrix Q, and applying a softmax function to obtain the weights on the values. They divided each query by √dk and multiplied it with the keys matrix K, then applied another softmax function to get the weights on the keys. Finally, they took the dot product of these weights with the values matrix V.<br><br>Source: Document 1, "data/Week 9 reading.pdf", id: f96c10482e0aa364ea5b285e155621d2                          | According to Document 3 and Document 7, researchers used attention in transformers by adding "positional" encoding to the inputs (Document 12). This means they incorporated a mechanism to account for the order of words or tokens in the input sequence, allowing the model to better understand the context and relationships between different parts of the text.<br><br>Additionally, as mentioned in Document 9, attention is a key component of transformers, enabling the model to focus on specific parts of the input when generating output. The attention mechanism allows the model to weigh the importance of different input elements and selectively attend to the most relevant ones.<br><br>Source: Documents 3, 7, and 9.                       |
| **Time Taken**         | 37.63 seconds                            | 13.44 seconds                         |

