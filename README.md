# Long Context Reordering Implementation Group 5

**Long Context Reordering** is a strategy for improving the ranking of documents. Traditional ranking approaches, such as those based only on relevance scores, put the most relevant documents at the top of the list and the least relevant documents at the bottom. However, this classic reranking method can sometimes lead to the loss of relatively relevant documents that are put in the middle of the list. As LLMs tend to look at documents at the top and bottom of the list. Therefore, we decided to use Long Context Reordering which puts the most relevant documents at the top and bottom of the list. For example, if there are 6 important documents out of 20, the first 3 documents will be put at the top of the ranking. Whereas the 4th, 5th and 6th relevant documents will be put at the bottom 3 positions of the ranking. This ensures that important information appears both at the beginning and the end of the list. 
In our case, we applied long context reordering to improve the performance with scoring documents based on their relevance to a given query. The goal was to ensure that key documents, those with high relevance scores, were strategically distributed across the ranked list. This would make them accessible for processing even in scenarios where not all documents could be analysed. 
To do this, we modified the compress_documents method in the ScoredCrossEncoderReranker class. After scoring and sorting documents by relevance using the CrossEncoder, we introduced a reordering step that altered between the top-ranked and bottom-ranked documents. This was achieved through a function called long_context_reorder, which processed the scored list and returned a reordered sequence of documents. This reordered list can then be used in the remainder of the RAG framework. 

## Evaluation
To evaluate if the ScoredCrossEncoderReranker model (reordered model) with reordering performs better than the Flashrank model from Langchain (baseline model), we calculated some measures. This was done with 49 documents (the course materials and also materials from other courses, to make up 49 documents). The measurements we used are: 
**1. Relevance Score:** A measure of how relevant each document is to the query. It is a raw measure for evaluating individual document relevance. The scores range between 0-1 (where 1= highly relevant, 0= not relevant). One limitation of this measurement is that it does not account for the position of documents in the ranking. That is why the next measurement is also calculated, which uses the relevance score. 
**2. Normalized Discounted Cumulative Gain (nDCG):** A ranking quality metric that considers the position of relevant documents in a ranked list. The nDCG score is in regards to the overall model, not each specific document like it is with the relevance score. It assigns higher importance to documents that are ranked earlier in the list (1=perfectly ranked, 0= ranking is completely irrelevant). The advantages are that it considers the ranking position, and that the normalization allows for comparison across queries of varying difficulty. 
**3. Latency:** Measures the time difference between the baseline and reordered system to ensure efficiency. The faster method is the most efficient, but of course it also depends on other factors, such as the measures above. 

## Results explanation 
The results of the measurements, which can be found in the Evaluation1.ipynb file, found for these cases differ a bit. First thing to note is that the documents that appeared after asking the same question were different for the two models, so that is already a difference with the reordered model. As far as the relevance score goes, for the baseline model it has overall higher scores, so in that aspect the baseline performs better. Then, using the relevance score, we found the nDCG scores for the models, and they were almost the same, but again the reordered model has a slightly lower score (difference is 1.0 for the baseline and 0.954 for the reordered). These results show that the baseline still performs a bit better than the reordered model. The last metric is latency, where the reordered model does run a little bit faster (roughly 5 seconds faster) than the baseline, so it does save some time. Thus, the reordered model does not perform as good as the baseline (but still not bad), in terms of relevance score and nDCG, but it is a little bit faster. This current approach may not have been better than the baseline model. However, there is potential for improvement of the Long Context Reordering model, which could eventually perform better than the baseline. 
